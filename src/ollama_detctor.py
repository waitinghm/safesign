import os
import time
from typing import List, Dict, Union
from dotenv import load_dotenv

# Ollama & DeepEval Imports
import ollama
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.models.base_model import DeepEvalBaseLLM
from deepeval.metrics.g_eval import Rubric
from deepeval.evaluate import AsyncConfig

# Project Modules
from law.legal_context import LawContextManager
from law.precedent_context import PrecedentContextManager

load_dotenv()

# --- 1. DeepEvalìš© Ollama ì–´ëŒ‘í„° (ollama.chat ì‚¬ìš©) ---
class OllamaDeepEvalAdapter(DeepEvalBaseLLM):
    """
    DeepEval í”„ë ˆì„ì›Œí¬ê°€ Ollama(Local LLM)ë¥¼ ì¸ì‹í•˜ê³  ì œì–´í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ì–´ëŒ‘í„°.
    LangChainì„ ê±°ì¹˜ì§€ ì•Šê³  ê³µì‹ ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ chat í•¨ìˆ˜ë¥¼ ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    def __init__(self, model_name="llama3"):
        self.model_name = model_name
        # ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë³„ë„ í´ë¼ì´ì–¸íŠ¸ ê°ì²´ ìƒì„± ë¶ˆí•„ìš”

    def load_model(self):
        return self.model_name

    def generate(self, prompt: str) -> str:
        """
        ê³µì‹ ollama.chat í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        try:
            response = ollama.chat(
                model=self.model_name,
                messages=[
                    {
                        "role": "user",
                        "content": prompt,
                    }
                ],
                stream=False
            )
            return response['message']['content']
        except Exception as e:
            return f"Ollama Generation Error: {e}"

    async def a_generate(self, prompt: str) -> str:
        return self.generate(prompt)

    def get_model_name(self):
        return self.model_name

# --- 2. ë…ì†Œì¡°í•­ íŒë³„ê¸° (Ollama ë²„ì „) ---
class ToxicClauseDetectorOllama:
    def __init__(self, model_name="llama3"):
        print(f"ğŸ›¡ï¸ ToxicClauseDetector (Ollama: {model_name}) ì´ˆê¸°í™” ì¤‘...")
        
        # Ollama ì–´ëŒ‘í„° ì—°ê²°
        self.evaluator_llm = OllamaDeepEvalAdapter(model_name=model_name)
        
        # DB ë§¤ë‹ˆì € ì´ˆê¸°í™” (RAG)
        self.law_manager = LawContextManager()
        self.law_manager.initialize_database()
        
        self.precedent_manager = PrecedentContextManager()
        self.precedent_manager.initialize_database()

        # [í‰ê°€ ê¸°ì¤€] - Gemini ë²„ì „ê³¼ ë™ì¼
        self.toxic_criteria = """
        ë‹¹ì‹ ì€ ê·¼ë¡œê³„ì•½ì„œì˜ [ë²•ì  íš¨ë ¥], [ê³µì •ì„±], [ëª…í™•ì„±]ì„ ì‹¬ì‚¬í•˜ëŠ” ì „ë¬¸ ë…¸ë¬´ AIì…ë‹ˆë‹¤.
        ì•„ë˜ 3ê°€ì§€ í•µì‹¬ ê¸°ì¤€ì— ë”°ë¼ ì¡°í•­ì„ ë¶„ì„í•˜ê³  ì ìˆ˜ë¥¼ ë§¤ê¸°ì„¸ìš”.

        [ë¶„ì„ ê¸°ì¤€ 3ëŒ€ ì›ì¹™]
        1. ë²•ì  íš¨ë ¥ (Legality) - [ì¹˜ëª…ì /Red Zone]
           - ê·¼ë¡œê¸°ì¤€ë²• ë“± ê°•í–‰ë²•ê·œë¥¼ ìœ„ë°˜í•˜ëŠ”ê°€?
           - ì˜ˆ: ìµœì €ì„ê¸ˆ ë¯¸ë‹¬, í‡´ì§ê¸ˆ í¬ê¸° ê°ì„œ, ìœ„ì•½ê¸ˆ ì˜ˆì •(ì†í•´ë°°ìƒì•¡ ëª…ì‹œ), í•´ê³  ì˜ˆê³  ìœ„ë°˜.
           - íŒë‹¨: ìœ„ë°˜ ì‹œ ë¬´ì¡°ê±´ 9~10ì  ë¶€ì—¬.

        2. ê³µì •ì„± (Fairness) - [ìœ„í—˜/Orange Zone]
           - ì‚¬ìš©ì(íšŒì‚¬)ì—ê²Œë§Œ ìœ ë¦¬í•˜ê³  ê·¼ë¡œìì—ê²Œ ê³¼ë„í•œ ì˜ë¬´ë¥¼ ë¶€ê³¼í•˜ëŠ”ê°€?
           - ì˜ˆ: "ëª¨ë“  ì†í•´ë¥¼ ë°°ìƒí•œë‹¤(í¬ê´„ì  ë°°ìƒ)", "í‡´ì‚¬ ì‹œ í›„ì„ìë¥¼ êµ¬í•´ì•¼ í•œë‹¤", "ì‚¬ë‚´ ê·œì • ìœ„ë°˜ ì‹œ ë¬´ì¡°ê±´ ì§•ê³„".
           - íŒë‹¨: ë¶ˆë²•ì€ ì•„ë‹ˆë”ë¼ë„ ê·¼ë¡œìê°€ ì–µìš¸í•  ì†Œì§€ê°€ í¬ë©´ 6~8ì  ë¶€ì—¬.

        3. ëª…í™•ì„± (Clarity) - [ì£¼ì˜/Yellow Zone]
           - ìì˜ì  í•´ì„ì´ ê°€ëŠ¥í•œ ëª¨í˜¸í•œ í‘œí˜„ì´ ìˆëŠ”ê°€?
           - ì˜ˆ: "íšŒì‚¬ê°€ í•„ìš”í•˜ë‹¤ê³  ì¸ì •í•˜ëŠ” ê²½ìš°", "ê´€ë¡€ì— ë”°ë¥¸ë‹¤", "ê¸°íƒ€ ê°‘ì´ ì •í•˜ëŠ” ì—…ë¬´".
           - íŒë‹¨: ë¬¸êµ¬ê°€ ëª¨í˜¸í•˜ì—¬ ë¶„ìŸ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©´ 3~5ì  ë¶€ì—¬.
        """
        
        self.rubric = [
            Rubric(score_range=(0, 2), expected_outcome="3ëŒ€ ì›ì¹™(íš¨ë ¥, ê³µì •ì„±, ëª…í™•ì„±)ì„ ëª¨ë‘ ì¶©ì¡±í•˜ëŠ” ì™„ë²½í•œ ì¡°í•­."),
            Rubric(score_range=(3, 5), expected_outcome="[ëª…í™•ì„± ë¶€ì¡±] - ë²•ì ìœ¼ë¡œ ë¬¸ì œëŠ” ì—†ìœ¼ë‚˜, í‘œí˜„ì´ ëª¨í˜¸í•˜ì—¬ íšŒì‚¬ì˜ ìì˜ì  í•´ì„ì´ ìš°ë ¤ë¨."),
            Rubric(score_range=(6, 8), expected_outcome="[ê³µì •ì„± ê²°ì—¬] - ë¶ˆë²• ì§ì „ì˜ ìˆ˜ì¤€. ê·¼ë¡œìì—ê²Œ ì¼ë°©ì ìœ¼ë¡œ ë¶ˆë¦¬í•˜ê±°ë‚˜ ì…ì¦ ì±…ì„ì„ ì „ê°€í•¨."),
            Rubric(score_range=(9, 10), expected_outcome="[ë²•ì  íš¨ë ¥ ì—†ìŒ] - ê·¼ë¡œê¸°ì¤€ë²• ê°•í–‰ê·œì • ìœ„ë°˜ìœ¼ë¡œ í•´ë‹¹ ì¡°í•­ ìì²´ê°€ ë¬´íš¨ì„."),
        ]

        self.evaluation_steps = [
            "1ë‹¨ê³„ [ì˜ë„ íŒŒì•…]: ì¡°í•­ì˜ í•µì‹¬ ì˜ë„(ì„ê¸ˆ ì‚­ê°, í•´ê³  ìš©ì´ì„±, ì±…ì„ ì „ê°€ ë“±)ë¥¼ ë¨¼ì € íŒŒì•…í•˜ê³ , ì¼ë°˜ì ì¸ ë²•ë¥  ì§€ì‹ì„ ë¡œë”©í•œë‹¤.",
            "2ë‹¨ê³„ [Legality/ì¹˜ëª…ì ]: ê·¼ë¡œê¸°ì¤€ë²• ê°•í–‰ê·œì • ìœ„ë°˜ ì—¬ë¶€ë¥¼ ìµœìš°ì„  í™•ì¸í•œë‹¤. íŠ¹íˆ 'í‡´ì§ê¸ˆ í¬ê¸°', 'ì†í•´ë°°ìƒì•¡ ì˜ˆì •', 'ê°•ì œ ê·¼ë¡œ' í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ 10ì ì„ ë¶€ì—¬í•œë‹¤.",
            "3ë‹¨ê³„ [Fairness/ìœ„í—˜]: ë¶ˆë²•ì´ ì•„ë‹ˆë¼ë©´, ê¶Œë¦¬ì™€ ì˜ë¬´ì˜ ê· í˜•ì„ ë³¸ë‹¤. íšŒì‚¬ì—ë§Œ ìœ ë¦¬í•˜ê±°ë‚˜ ê·¼ë¡œìì—ê²Œ ê³¼ë„í•œ ì˜ë¬´ë¥¼ ë¶€ê³¼í•˜ë©´ 6~8ì ì„ ë¶€ì—¬í•œë‹¤.",
            "4ë‹¨ê³„ [Clarity/ì£¼ì˜]: ë‚´ìš©ì´ ê³µì •í•´ ë³´ì—¬ë„, 'ê¸°íƒ€', 'ìƒë‹¹í•œ' ë“± ìì˜ì  í•´ì„ì´ ê°€ëŠ¥í•œ ëª¨í˜¸í•œ ë‹¨ì–´ê°€ ìˆë‹¤ë©´ 3~5ì ì„ ë¶€ì—¬í•œë‹¤.",
            "5ë‹¨ê³„ [ì¢…í•© íŒë‹¨]: ìœ„ ë‹¨ê³„ë“¤ì„ ê±°ì³ ì ìˆ˜ë¥¼ ë§¤ê¸°ë˜, ë²•ì  ê·¼ê±°ê°€ í™•ì‹¤í•˜ì§€ ì•Šì€ íšŒìƒ‰ì§€ëŒ€ë¼ë©´ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•œ ìª½(ë³´ìˆ˜ì )ìœ¼ë¡œ í•´ì„í•˜ì—¬ ìµœì¢… ì ìˆ˜ë¥¼ í™•ì •í•œë‹¤.",
        ]
        
        # G-Eval Metric ê°ì²´ ìƒì„±
        self.toxic_metric = GEval(
            name="Toxicity Score (Ollama)",
            criteria=self.toxic_criteria,
            rubric=self.rubric,
            evaluation_steps=self.evaluation_steps,
            model=self.evaluator_llm, # Ollamaê°€ ì‹¬ì‚¬ìœ„ì›
            threshold=5, 
            evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT]
        )

    def _retrieve_context(self, clause_text):
        # 1. ë²•ë ¹ ê²€ìƒ‰
        laws = self.law_manager.search_relevant_laws(clause_text, k=2)
        law_text = "\n".join(laws) if laws else "ê´€ë ¨ ë²•ë ¹ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"

        # 2. íŒë¡€ ê²€ìƒ‰
        precedents = self.precedent_manager.search_relevant_precedents(clause_text, k=1)
        precedent_text = precedents[0] if precedents else "ê´€ë ¨ íŒë¡€ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"

        return f"=== [ê´€ë ¨ ë²•ë ¹] ===\n{law_text}\n\n=== [ê´€ë ¨ íŒë¡€] ===\n{precedent_text}"

    def detect(self, clause_texts: List[str], max_concurrent: int = 1) -> List[Dict]:
        """
        Ollamaì˜ JSON íŒŒì‹± ì˜¤ë¥˜ë‚˜ ë¶ˆì•ˆì •ì„±ì„ ê³ ë ¤í•˜ì—¬ evaluate í•¨ìˆ˜ ëŒ€ì‹  ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        """
        print(f"ğŸš€ ì´ {len(clause_texts)}ê°œ ì¡°í•­ì— ëŒ€í•œ í‰ê°€ ì‹œì‘ (Ollama)...")
        
        formatted_results = []
        original_map = {} 

        # ìˆœì°¨ ì²˜ë¦¬ Loop
        for i, text in enumerate(clause_texts):
            print(f"   Processing Clause {i+1}/{len(clause_texts)}...", end="\r")
            
            # 1. RAG ê²€ìƒ‰
            retrieved_context = self._retrieve_context(text)
            original_map[text] = retrieved_context
            
            # 2. Test Case ìƒì„±
            test_case = LLMTestCase(
                input=text,
                actual_output="í‰ê°€ ëŒ€ìƒ",
                retrieval_context=[retrieved_context]
            )

            # 3. í‰ê°€ ì‹¤í–‰ (Try-Exceptë¡œ ë³´í˜¸)
            try:
                self.toxic_metric.measure(test_case)
                
                # ì„±ê³µ ì‹œ ë°ì´í„° ì¶”ì¶œ
                metric_score = self.toxic_metric.score
                metric_reason = self.toxic_metric.reason
                
                # ì ìˆ˜ ë³´ì • (0.0~1.0 -> 0~10)
                risk_score = metric_score
                if risk_score <= 1.0:
                    risk_score *= 10
                
                is_toxic = risk_score >= 4.0

            except Exception as e:
                # ì‹¤íŒ¨ ì‹œ
                print(f"\nâš ï¸ [Skip Clause {i+1}] ëª¨ë¸ ì‘ë‹µ ì˜¤ë¥˜: {e}")
                risk_score = 0
                is_toxic = False
                metric_reason = f"Ollama ëª¨ë¸ ì¶œë ¥ ì˜¤ë¥˜ (JSON Parsing Failed): {e}"

            # ê²°ê³¼ ì €ì¥
            formatted_results.append({
                "clause": text,
                "is_toxic": is_toxic,
                "risk_score": round(risk_score, 1),
                "reason": metric_reason,
                "context_used": retrieved_context
            })

        print("\nâœ… ëª¨ë“  í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return formatted_results

    def generate_easy_suggestion(self, detection_result):
        """Ollamaë¥¼ ì´ìš©í•´ ì‰¬ìš´ í•´ì„ ë° ìˆ˜ì • ì œì•ˆ ìƒì„±"""
        if not detection_result['is_toxic']:
            return "âœ… **ì•ˆì „í•œ ì¡°í•­ì…ë‹ˆë‹¤.**"

        prompt = f"""
        ë‹¹ì‹ ì€ ê·¼ë¡œì í¸ì¸ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë…ì†Œì¡°í•­ì„ ë¶„ì„í•˜ì„¸ìš”.
        
        [ì›ë¬¸]: {detection_result['clause']}
        [ìœ„í—˜ íŒë‹¨ ì´ìœ ]: {detection_result['reason']}
        [ë²•ì  ê·¼ê±°]: {detection_result['context_used']}

        ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš” (í•œêµ­ì–´):
        1. **âš ï¸ ì‰¬ìš´ í•´ì„**: ê·¼ë¡œìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½.
        2. **ğŸ’¡ ìˆ˜ì • ì œì•ˆ**: ë²•ì— ë§ëŠ” ê³µì •í•œ ì¡°í•­ ì˜ˆì‹œ.
        """
        return self.evaluator_llm.generate(prompt)

# --- ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ---
if __name__ == "__main__":
    try:
        # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ëª… ì„¤ì •
        TARGET_MODEL = "hf.co/LiquidAI/LFM2-8B-A1B-GGUF:Q4_K_M"
        
        detector = ToxicClauseDetectorOllama(model_name=TARGET_MODEL)
        
        test_clauses = [
            "í‡´ì‚¬ ì‹œ í›„ì„ìë¥¼ êµ¬í•˜ì§€ ëª»í•˜ë©´ ê·¸ë¡œ ì¸í•œ ëª¨ë“  ì†í•´ë¥¼ ë°°ìƒí•´ì•¼ í•œë‹¤.",
            "ìˆ˜ìŠµê¸°ê°„ 3ê°œì›” ë™ì•ˆì€ ìµœì €ì„ê¸ˆì˜ 80%ë§Œ ì§€ê¸‰í•œë‹¤."
        ]
        
        # ë¡œì»¬ ëª¨ë¸ì€ ëŠë¦¬ë¯€ë¡œ max_concurrentë¥¼ ì‘ê²Œ ì„¤ì • (ì‹¤ì œ ë¡œì§ì€ ìˆœì°¨ ì²˜ë¦¬ë¨)
        results = detector.detect(test_clauses, max_concurrent=1)
        
        print("\n" + "="*50)
        for res in results:
            icon = "ğŸš¨" if res['is_toxic'] else "âœ…"
            print(f"{icon} ì ìˆ˜: {res['risk_score']} | ë‚´ìš©: {res['clause'][:30]}...")
            print(f"   ì´ìœ : {res['reason']}")
            
            if res['is_toxic']:
                print("\n   [AI ì œì•ˆ ìƒì„± ì¤‘...]")
                suggestion = detector.generate_easy_suggestion(res)
                print(suggestion)
            print("-" * 50)
            
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")