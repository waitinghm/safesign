import os
from dotenv import load_dotenv
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.models.base_model import DeepEvalBaseLLM
from deepeval.metrics.g_eval import Rubric

# [Import]
from llm_service import LLM_gemini
from law.legal_context import LawContextManager
from law.precedent_context import PrecedentContextManager

#load_dotenv()

# --- 1. DeepEvalìš© Gemini ì–´ëŒ‘í„° ---
class GeminiDeepEvalAdapter(DeepEvalBaseLLM):
    def __init__(self, llm_service: LLM_gemini):
        self.llm_service = llm_service
        self.model_name = llm_service.model_name

    def load_model(self):
        return self.llm_service.client

    def generate(self, prompt: str) -> str:
        response = self.llm_service.generate(prompt)
        return response.text

    async def a_generate(self, prompt: str) -> str:
        return self.generate(prompt)

    def get_model_name(self):
        return self.model_name

# --- 2. ë…ì†Œì¡°í•­ íŒë³„ê¸° í´ë˜ìŠ¤ ---
class ToxicClauseDetector:
    def __init__(self, api_key=None):
        print("ğŸ›¡ï¸ ToxicClauseDetector (Pro Model) ì´ˆê¸°í™” ì¤‘...")
        
        if not api_key:
            api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("Gemini API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")

        
        self.llm_service = LLM_gemini(gemini_api_key=api_key, model="gemini-2.5-flash")
        self.evaluator_llm = GeminiDeepEvalAdapter(self.llm_service)
        
        # DB ë§¤ë‹ˆì €
        self.law_manager = LawContextManager()
        self.precedent_manager = PrecedentContextManager()
        self.law_manager.initialize_database()
        self.precedent_manager.initialize_database()

       # 1. 3ëŒ€ ì›ì¹™(Legality, Fairness, Clarity)ì„ í•µì‹¬ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
        # ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ 'ìœ„í—˜(Toxic)'í•œ ê²ƒìœ¼ë¡œ ê¸°ì¤€ì„ ë’¤ì§‘ìŠµë‹ˆë‹¤.
        self.toxic_criteria = """
       ë‹¹ì‹ ì€ ê·¼ë¡œê³„ì•½ì„œì˜ [ë²•ì  íš¨ë ¥], [ê³µì •ì„±], [ëª…í™•ì„±]ì„ ì‹¬ì‚¬í•˜ëŠ” ì „ë¬¸ ë…¸ë¬´ AIì…ë‹ˆë‹¤.
        ì•„ë˜ 3ê°€ì§€ í•µì‹¬ ê¸°ì¤€ì— ë”°ë¼ ì¡°í•­ì„ ë¶„ì„í•˜ê³  ì ìˆ˜ë¥¼ ë§¤ê¸°ì„¸ìš”.

        [ë¶„ì„ ê¸°ì¤€ 3ëŒ€ ì›ì¹™]
        1. ë²•ì  íš¨ë ¥ (Legality) - [ì¹˜ëª…ì /Red Zone]
           - ê·¼ë¡œê¸°ì¤€ë²• ë“± ê°•í–‰ë²•ê·œë¥¼ ìœ„ë°˜í•˜ëŠ”ê°€?
           - ì˜ˆ: ìµœì €ì„ê¸ˆ ë¯¸ë‹¬, í‡´ì§ê¸ˆ í¬ê¸° ê°ì„œ, ìœ„ì•½ê¸ˆ ì˜ˆì •(ì†í•´ë°°ìƒì•¡ ëª…ì‹œ), í•´ê³  ì˜ˆê³  ìœ„ë°˜.
           - íŒë‹¨: ìœ„ë°˜ ì‹œ ë¬´ì¡°ê±´ 9~10ì  ë¶€ì—¬.

        2. ê³µì •ì„± (Fairness) - [ìœ„í—˜/Orange Zone]
           - ì‚¬ìš©ì(íšŒì‚¬)ì—ê²Œë§Œ ìœ ë¦¬í•˜ê³  ê·¼ë¡œìì—ê²Œ ê³¼ë„í•œ ì˜ë¬´ë¥¼ ë¶€ê³¼í•˜ëŠ”ê°€?
           - ì˜ˆ: "ëª¨ë“  ì†í•´ë¥¼ ë°°ìƒí•œë‹¤(í¬ê´„ì  ë°°ìƒ)", "í‡´ì‚¬ ì‹œ í›„ì„ìë¥¼ êµ¬í•´ì•¼ í•œë‹¤", "ì‚¬ë‚´ ê·œì • ìœ„ë°˜ ì‹œ ë¬´ì¡°ê±´ ì§•ê³„".
           - íŒë‹¨: ë¶ˆë²•ì€ ì•„ë‹ˆë”ë¼ë„ ê·¼ë¡œìê°€ ì–µìš¸í•  ì†Œì§€ê°€ í¬ë©´ 6~8ì  ë¶€ì—¬.

        3. ëª…í™•ì„± (Clarity) - [ì£¼ì˜/Yellow Zone]
           - ìì˜ì  í•´ì„ì´ ê°€ëŠ¥í•œ ëª¨í˜¸í•œ í‘œí˜„ì´ ìˆëŠ”ê°€?
           - ì˜ˆ: "íšŒì‚¬ê°€ í•„ìš”í•˜ë‹¤ê³  ì¸ì •í•˜ëŠ” ê²½ìš°", "ê´€ë¡€ì— ë”°ë¥¸ë‹¤", "ê¸°íƒ€ ê°‘ì´ ì •í•˜ëŠ” ì—…ë¬´".
           - íŒë‹¨: ë¬¸êµ¬ê°€ ëª¨í˜¸í•˜ì—¬ ë¶„ìŸ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©´ 3~5ì  ë¶€ì—¬.
        """
        # 2. ê°œì„ ëœ ì±„ì  ë£¨ë¸Œë¦­ (Rubric)
        # - ì ìˆ˜ëŒ€ë³„ë¡œ 'ëª…ë°±í•œ ë¶ˆë²•'ê³¼ 'í•´ì„ìƒ ë¶ˆë¦¬í•¨'ì„ í™•ì‹¤íˆ êµ¬ë¶„
        self.rubric = [
            Rubric(score_range=(0, 2), expected_outcome="3ëŒ€ ì›ì¹™(íš¨ë ¥, ê³µì •ì„±, ëª…í™•ì„±)ì„ ëª¨ë‘ ì¶©ì¡±í•˜ëŠ” ì™„ë²½í•œ ì¡°í•­."),
            Rubric(score_range=(3, 5), expected_outcome="[ëª…í™•ì„± ë¶€ì¡±] - ë²•ì ìœ¼ë¡œ ë¬¸ì œëŠ” ì—†ìœ¼ë‚˜, í‘œí˜„ì´ ëª¨í˜¸í•˜ì—¬ íšŒì‚¬ì˜ ìì˜ì  í•´ì„ì´ ìš°ë ¤ë¨."),
            Rubric(score_range=(6, 8), expected_outcome="[ê³µì •ì„± ê²°ì—¬] - ë¶ˆë²• ì§ì „ì˜ ìˆ˜ì¤€. ê·¼ë¡œìì—ê²Œ ì¼ë°©ì ìœ¼ë¡œ ë¶ˆë¦¬í•˜ê±°ë‚˜ ì…ì¦ ì±…ì„ì„ ì „ê°€í•¨."),
            Rubric(score_range=(9, 10), expected_outcome="[ë²•ì  íš¨ë ¥ ì—†ìŒ] - ê·¼ë¡œê¸°ì¤€ë²• ê°•í–‰ê·œì • ìœ„ë°˜ìœ¼ë¡œ í•´ë‹¹ ì¡°í•­ ìì²´ê°€ ë¬´íš¨ì„."),
        ]

        self.evaluation_steps = [
            "0ë‹¨ê³„ [ì˜ë„ íŒŒì•…]: ì¡°í•­ì˜ í•µì‹¬ ì˜ë„(ì„ê¸ˆ ì‚­ê°, í•´ê³  ìš©ì´ì„±, ì±…ì„ ì „ê°€ ë“±)ë¥¼ ë¨¼ì € íŒŒì•…í•˜ê³ , ì¼ë°˜ì ì¸ ë²•ë¥  ì§€ì‹ì„ ë¡œë”©í•œë‹¤.",
            "1ë‹¨ê³„ [Legality/ì¹˜ëª…ì ]: ê·¼ë¡œê¸°ì¤€ë²• ê°•í–‰ê·œì • ìœ„ë°˜ ì—¬ë¶€ë¥¼ ìµœìš°ì„  í™•ì¸í•œë‹¤. íŠ¹íˆ 'í‡´ì§ê¸ˆ í¬ê¸°', 'ì†í•´ë°°ìƒì•¡ ì˜ˆì •', 'ê°•ì œ ê·¼ë¡œ' í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ 10ì ì„ ë¶€ì—¬í•œë‹¤.",
            "2ë‹¨ê³„ [Fairness/ìœ„í—˜]: ë¶ˆë²•ì´ ì•„ë‹ˆë¼ë©´, ê¶Œë¦¬ì™€ ì˜ë¬´ì˜ ê· í˜•ì„ ë³¸ë‹¤. íšŒì‚¬ì—ë§Œ ìœ ë¦¬í•˜ê±°ë‚˜ ê·¼ë¡œìì—ê²Œ ê³¼ë„í•œ ì˜ë¬´ë¥¼ ë¶€ê³¼í•˜ë©´ 6~8ì ì„ ë¶€ì—¬í•œë‹¤.",
            "3ë‹¨ê³„ [Clarity/ì£¼ì˜]: ë‚´ìš©ì´ ê³µì •í•´ ë³´ì—¬ë„, 'ê¸°íƒ€', 'ìƒë‹¹í•œ' ë“± ìì˜ì  í•´ì„ì´ ê°€ëŠ¥í•œ ëª¨í˜¸í•œ ë‹¨ì–´ê°€ ìˆë‹¤ë©´ 3~5ì ì„ ë¶€ì—¬í•œë‹¤.",
            "4ë‹¨ê³„ [ì¢…í•© íŒë‹¨]: ìœ„ ë‹¨ê³„ë“¤ì„ ê±°ì³ ì ìˆ˜ë¥¼ ë§¤ê¸°ë˜, ë²•ì  ê·¼ê±°ê°€ í™•ì‹¤í•˜ì§€ ì•Šì€ íšŒìƒ‰ì§€ëŒ€ë¼ë©´ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•œ ìª½(ë³´ìˆ˜ì )ìœ¼ë¡œ í•´ì„í•˜ì—¬ ìµœì¢… ì ìˆ˜ë¥¼ í™•ì •í•œë‹¤."
        ]

    def _retrieve_context(self, clause_text):
        # 1. ë²•ë ¹ ê²€ìƒ‰
        laws = self.law_manager.search_relevant_laws(clause_text, k=2)
        law_text = "\n".join(laws) if laws else "ê´€ë ¨ ë²•ë ¹ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ì¼ë°˜ ë²•ë¥  ì§€ì‹ìœ¼ë¡œ íŒë‹¨ ìš”ë§)"

        # 2. íŒë¡€ ê²€ìƒ‰
        precedents = self.precedent_manager.search_relevant_precedents(clause_text, k=1)
        precedent_text = precedents[0] if precedents else "ê´€ë ¨ íŒë¡€ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"

        return f"=== [ê´€ë ¨ ë²•ë ¹] ===\n{law_text}\n\n=== [ê´€ë ¨ íŒë¡€] ===\n{precedent_text}"

    def detect(self, clause_text):
        # print(f"ğŸ•µï¸ ì¡°í•­ ë¶„ì„ ì¤‘: {clause_text[:30]}...")
        
        retrieved_context = self._retrieve_context(clause_text)
        
        # G-Eval í‰ê°€
        toxic_metric = GEval(
            name="Toxicity Score", # ì´ë¦„ ë³€ê²½
            criteria=self.toxic_criteria,
            rubric=self.rubric,
            evaluation_steps=self.evaluation_steps,
            model=self.evaluator_llm, 
            threshold=5, # 5ì  ì´ìƒì´ë©´ ë…ì†Œì¡°í•­ìœ¼ë¡œ ê°„ì£¼
            evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT]
        )

        test_case = LLMTestCase(
            input=clause_text,
            actual_output="í‰ê°€ ëŒ€ìƒ",
            retrieval_context=[retrieved_context]
        )

        toxic_metric.measure(test_case)
        
        # ì´ì œ ì ìˆ˜(0~10)ê°€ ê³§ ìœ„í—˜ë„ì…ë‹ˆë‹¤. ë’¤ì§‘ì„ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
        risk_score = toxic_metric.score # 0~10ì  (DeepEval ë²„ì „ì— ë”°ë¼ 0~1ì¼ ìˆ˜ë„ ìˆìŒ, ì•„ë˜ ë³´ì •)
        
        # DeepEvalì´ 0~1 ì‚¬ì´ ê°’ì„ ë¦¬í„´í•˜ëŠ” ê²½ìš° 10ì„ ê³±í•´ì¤Œ
        if risk_score <= 1.0:
            risk_score *= 10
            
        # 4ì  ì´ìƒì´ë©´ ë…ì†Œì¡°í•­ (ê¸°ì¤€ ê°•í™”)
        is_toxic = risk_score >= 4.0
        
        # ë””ë²„ê¹…ìš© ì¶œë ¥ (í„°ë¯¸ë„ì—ì„œ í™•ì¸ ê°€ëŠ¥)
        print(f"[{'ğŸš¨ìœ„í—˜' if is_toxic else 'âœ…ì•ˆì „'}] ì ìˆ˜: {risk_score} | ë‚´ìš©: {clause_text[:20]}...")

        return {
            "clause": clause_text,
            "is_toxic": is_toxic,
            "risk_score": round(risk_score, 1),
            "reason": toxic_metric.reason,
            "context_used": retrieved_context
        }

    def generate_easy_suggestion(self, detection_result):
        if not detection_result['is_toxic']:
            return "âœ… **ì•ˆì „í•œ ì¡°í•­ì…ë‹ˆë‹¤.**"

        prompt = f"""
        ë‹¹ì‹ ì€ ê·¼ë¡œì í¸ì¸ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë…ì†Œì¡°í•­ì„ ë¶„ì„í•˜ì„¸ìš”.
        
        [ì›ë¬¸]: {detection_result['clause']}
        [ì´ìœ ]: {detection_result['reason']}
        [ê·¼ê±°]: {detection_result['context_used']}

        ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì‘ì„±:
        1. **âš ï¸ ì‰¬ìš´ í•´ì„**: ê·¼ë¡œìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ 1~2ë¬¸ì¥ìœ¼ë¡œ 'ì™œ ìœ„í—˜í•œì§€' ì„¤ëª… ë° ìš”ì•½.
        2. **ğŸ’¡ ìˆ˜ì • ì œì•ˆ**: ë²•ì— ë§ëŠ” ê³µì •í•œ ì¡°í•­ ì˜ˆì‹œ.
        """
        response = self.llm_service.generate(prompt)
        return response.text