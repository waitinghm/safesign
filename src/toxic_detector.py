# Copyright (c) 2025 SafeSign Team
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
import os
import time
from typing import List, Dict, Union
from dotenv import load_dotenv

# DeepEval Imports
from deepeval import evaluate
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.models.base_model import DeepEvalBaseLLM
from deepeval.metrics.g_eval import Rubric
from deepeval.evaluate import AsyncConfig

from llm_service import LLM_gemini
from law.legal_context import LawContextManager
from law.precedent_context import PrecedentContextManager

# --- 1. DeepEvalìš© Gemini ì–´ëŒ‘í„° ---
class GeminiDeepEvalAdapter(DeepEvalBaseLLM):
    def __init__(self, llm_service: LLM_gemini):
        self.llm_service = llm_service
        self.model_name = llm_service.model_name

    def load_model(self):
        return self.llm_service.client

    def generate(self, prompt: str) -> str:
        response = self.llm_service.generate(prompt)
        return response.text if hasattr(response, 'text') else str(response)

    async def a_generate(self, prompt: str) -> str:
        return self.generate(prompt)

    def get_model_name(self):
        return self.model_name

# --- 2. ë…ì†Œì¡°í•­ íŒë³„ê¸° í´ë˜ìŠ¤ ---
class ToxicClauseDetector:
    def __init__(self, api_key=None):
        print("ğŸ›¡ï¸ ToxicClauseDetector (Parallel) ì´ˆê¸°í™” ì¤‘...")
        
        if not api_key:
            api_key = os.getenv("GEMINI_API_KEY")
        
        self.llm_service = LLM_gemini(gemini_api_key=api_key, model="gemini-2.5-flash-lite")
        self.evaluator_llm = GeminiDeepEvalAdapter(self.llm_service)
        
        # DB ë§¤ë‹ˆì €
        self.law_manager = LawContextManager()
        self.precedent_manager = PrecedentContextManager()
        self.law_manager.initialize_database()
        self.precedent_manager.initialize_database()

        # [User Original Prompt & Logic] - ìˆ˜ì •í•˜ì§€ ì•ŠìŒ
        self.toxic_criteria = """
       ë‹¹ì‹ ì€ ê·¼ë¡œê³„ì•½ì„œì˜ [ë²•ì  íš¨ë ¥], [ê³µì •ì„±], [ëª…í™•ì„±]ì„ ì‹¬ì‚¬í•˜ëŠ” ì „ë¬¸ ë…¸ë¬´ AIì…ë‹ˆë‹¤.
        ì•„ë˜ 3ê°€ì§€ í•µì‹¬ ê¸°ì¤€ì— ë”°ë¼ ì¡°í•­ì„ ë¶„ì„í•˜ê³  ì ìˆ˜ë¥¼ ë§¤ê¸°ì„¸ìš”.

        [ë¶„ì„ ê¸°ì¤€ 3ëŒ€ ì›ì¹™]
        1. ë²•ì  íš¨ë ¥ (Legality) - [ì¹˜ëª…ì /Red Zone]
           - ê·¼ë¡œê¸°ì¤€ë²• ë“± ê°•í–‰ë²•ê·œë¥¼ ìœ„ë°˜í•˜ëŠ”ê°€?
           - ì˜ˆ: ìµœì €ì„ê¸ˆ ë¯¸ë‹¬, í‡´ì§ê¸ˆ í¬ê¸° ê°ì„œ, ìœ„ì•½ê¸ˆ ì˜ˆì •(ì†í•´ë°°ìƒì•¡ ëª…ì‹œ), í•´ê³  ì˜ˆê³  ìœ„ë°˜.
           - íŒë‹¨: ìœ„ë°˜ ì‹œ ë¬´ì¡°ê±´ 9~10ì  ë¶€ì—¬.

        2. ê³µì •ì„± (Fairness) - [ìœ„í—˜/Orange Zone]
           - ì‚¬ìš©ì(íšŒì‚¬)ì—ê²Œë§Œ ìœ ë¦¬í•˜ê³  ê·¼ë¡œìì—ê²Œ ê³¼ë„í•œ ì˜ë¬´ë¥¼ ë¶€ê³¼í•˜ëŠ”ê°€?
           - ì˜ˆ: "ëª¨ë“  ì†í•´ë¥¼ ë°°ìƒí•œë‹¤(í¬ê´„ì  ë°°ìƒ)", "í‡´ì‚¬ ì‹œ í›„ì„ìë¥¼ êµ¬í•´ì•¼ í•œë‹¤", "ì‚¬ë‚´ ê·œì • ìœ„ë°˜ ì‹œ ë¬´ì¡°ê±´ ì§•ê³„".
           - íŒë‹¨: ë¶ˆë²•ì€ ì•„ë‹ˆë”ë¼ë„ ê·¼ë¡œìê°€ ì–µìš¸í•  ì†Œì§€ê°€ í¬ë©´ 6~8ì  ë¶€ì—¬.

        3. ëª…í™•ì„± (Clarity) - [ì£¼ì˜/Yellow Zone]
           - ìì˜ì  í•´ì„ì´ ê°€ëŠ¥í•œ ëª¨í˜¸í•œ í‘œí˜„ì´ ìˆëŠ”ê°€?
           - ì˜ˆ: "íšŒì‚¬ê°€ í•„ìš”í•˜ë‹¤ê³  ì¸ì •í•˜ëŠ” ê²½ìš°", "ê´€ë¡€ì— ë”°ë¥¸ë‹¤", "ê¸°íƒ€ ê°‘ì´ ì •í•˜ëŠ” ì—…ë¬´".
           - íŒë‹¨: ë¬¸êµ¬ê°€ ëª¨í˜¸í•˜ì—¬ ë¶„ìŸ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©´ 3~5ì  ë¶€ì—¬.
        """
        
        self.rubric = [
            Rubric(score_range=(0, 2), expected_outcome="3ëŒ€ ì›ì¹™(íš¨ë ¥, ê³µì •ì„±, ëª…í™•ì„±)ì„ ëª¨ë‘ ì¶©ì¡±í•˜ëŠ” ì™„ë²½í•œ ì¡°í•­."),
            Rubric(score_range=(3, 5), expected_outcome="[ëª…í™•ì„± ë¶€ì¡±] - ë²•ì ìœ¼ë¡œ ë¬¸ì œëŠ” ì—†ìœ¼ë‚˜, í‘œí˜„ì´ ëª¨í˜¸í•˜ì—¬ íšŒì‚¬ì˜ ìì˜ì  í•´ì„ì´ ìš°ë ¤ë¨."),
            Rubric(score_range=(6, 8), expected_outcome="[ê³µì •ì„± ê²°ì—¬] - ë¶ˆë²• ì§ì „ì˜ ìˆ˜ì¤€. ê·¼ë¡œìì—ê²Œ ì¼ë°©ì ìœ¼ë¡œ ë¶ˆë¦¬í•˜ê±°ë‚˜ ì…ì¦ ì±…ì„ì„ ì „ê°€í•¨."),
            Rubric(score_range=(9, 10), expected_outcome="[ë²•ì  íš¨ë ¥ ì—†ìŒ] - ê·¼ë¡œê¸°ì¤€ë²• ê°•í–‰ê·œì • ìœ„ë°˜ìœ¼ë¡œ í•´ë‹¹ ì¡°í•­ ìì²´ê°€ ë¬´íš¨ì„."),
        ]

        self.evaluation_steps = [
            "1ë‹¨ê³„ [ì˜ë„ íŒŒì•…]: ì¡°í•­ì˜ í•µì‹¬ ì˜ë„(ì„ê¸ˆ ì‚­ê°, í•´ê³  ìš©ì´ì„±, ì±…ì„ ì „ê°€ ë“±)ë¥¼ ë¨¼ì € íŒŒì•…í•˜ê³ , ì¼ë°˜ì ì¸ ë²•ë¥  ì§€ì‹ì„ ë¡œë”©í•œë‹¤.",
            "2ë‹¨ê³„ [Legality/ì¹˜ëª…ì ]: ê·¼ë¡œê¸°ì¤€ë²• ê°•í–‰ê·œì • ìœ„ë°˜ ì—¬ë¶€ë¥¼ ìµœìš°ì„  í™•ì¸í•œë‹¤. íŠ¹íˆ 'í‡´ì§ê¸ˆ í¬ê¸°', 'ì†í•´ë°°ìƒì•¡ ì˜ˆì •', 'ê°•ì œ ê·¼ë¡œ' í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ 10ì ì„ ë¶€ì—¬í•œë‹¤.",
            "3ë‹¨ê³„ [Fairness/ìœ„í—˜]: ë¶ˆë²•ì´ ì•„ë‹ˆë¼ë©´, ê¶Œë¦¬ì™€ ì˜ë¬´ì˜ ê· í˜•ì„ ë³¸ë‹¤. íšŒì‚¬ì—ë§Œ ìœ ë¦¬í•˜ê±°ë‚˜ ê·¼ë¡œìì—ê²Œ ê³¼ë„í•œ ì˜ë¬´ë¥¼ ë¶€ê³¼í•˜ë©´ 6~8ì ì„ ë¶€ì—¬í•œë‹¤.",
            "4ë‹¨ê³„ [Clarity/ì£¼ì˜]: ë‚´ìš©ì´ ê³µì •í•´ ë³´ì—¬ë„, 'ê¸°íƒ€', 'ìƒë‹¹í•œ' ë“± ìì˜ì  í•´ì„ì´ ê°€ëŠ¥í•œ ëª¨í˜¸í•œ ë‹¨ì–´ê°€ ìˆë‹¤ë©´ 3~5ì ì„ ë¶€ì—¬í•œë‹¤.",
            "5ë‹¨ê³„ [ì¢…í•© íŒë‹¨]: ìœ„ ë‹¨ê³„ë“¤ì„ ê±°ì³ ì ìˆ˜ë¥¼ ë§¤ê¸°ë˜, ë²•ì  ê·¼ê±°ê°€ í™•ì‹¤í•˜ì§€ ì•Šì€ íšŒìƒ‰ì§€ëŒ€ë¼ë©´ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•œ ìª½(ë³´ìˆ˜ì )ìœ¼ë¡œ í•´ì„í•˜ì—¬ ìµœì¢… ì ìˆ˜ë¥¼ í™•ì •í•œë‹¤.",

        ]
        
        # Metric ê°ì²´ ì´ˆê¸°í™” (ì¬ì‚¬ìš©)
        self.toxic_metric = GEval(
            name="Toxicity Score",
            criteria=self.toxic_criteria,
            rubric=self.rubric,
            evaluation_steps=self.evaluation_steps,
            model=self.evaluator_llm,
            threshold=5, 
            evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT]
        )

    def _retrieve_context(self, clause_text):
        laws = self.law_manager.search_relevant_laws(clause_text, k=2)
        law_text = "\n".join(laws) if laws else "ê´€ë ¨ ë²•ë ¹ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ì¼ë°˜ ë²•ë¥  ì§€ì‹ìœ¼ë¡œ íŒë‹¨ ìš”ë§)"

        precedents = self.precedent_manager.search_relevant_precedents(clause_text, k=1)
        precedent_text = precedents[0] if precedents else "ê´€ë ¨ íŒë¡€ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"

        return f"=== [ê´€ë ¨ ë²•ë ¹] ===\n{law_text}\n\n=== [ê´€ë ¨ íŒë¡€] ===\n{precedent_text}"

    # í•¨ìˆ˜ëª… 'detect' ìœ ì§€ (Input: List[str]ë¡œ ë³€ê²½ë¨)
    def detect(self, clause_texts: List[str], max_concurrent: int = 5) -> List[Dict]:
        """
        [ë³‘ë ¬ ì²˜ë¦¬] ì—¬ëŸ¬ ì¡°í•­ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ì•„ DeepEval evaluate í•¨ìˆ˜ë¡œ í•œ ë²ˆì— ì²˜ë¦¬.
        """
        print(f"ğŸš€ ì´ {len(clause_texts)}ê°œ ì¡°í•­ì— ëŒ€í•œ ë³‘ë ¬ í‰ê°€ ì‹œì‘...")
        
        test_cases = []
        original_map = {} # ê²°ê³¼ ë§¤í•‘ìš©

        # 1. Test Case ìƒì„± (Retrieval ìˆ˜í–‰)
        for text in clause_texts:
            retrieved_context = self._retrieve_context(text)
            test_case = LLMTestCase(
                input=text,
                actual_output="í‰ê°€ ëŒ€ìƒ",
                retrieval_context=[retrieved_context]
            )
            test_cases.append(test_case)
            original_map[text] = retrieved_context
        # 2. ë³‘ë ¬ í‰ê°€ ì‹¤í–‰ (evaluate)
        eval_results = evaluate(
            test_cases=test_cases,
            metrics=[self.toxic_metric],
            async_config=AsyncConfig(max_concurrent=max_concurrent), # ë³‘ë ¬ ì²˜ë¦¬ ê°œìˆ˜
        )

        # 3. ê²°ê³¼ í¬ë§·íŒ… (ìˆ˜ì •ëœ ë¡œì§)
        formatted_results = []
        
        # [í•µì‹¬] eval_results ê°ì²´ì—ì„œ ì§„ì§œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸(.test_results)ë¥¼ êº¼ëƒ…ë‹ˆë‹¤.
        if hasattr(eval_results, 'test_results'):
            actual_test_results = eval_results.test_results
        elif isinstance(eval_results, list):
            actual_test_results = eval_results
        else:
            # í˜¹ì‹œ ëª¨ë¥¼ ìƒí™© ëŒ€ë¹„ (ë”•ì…”ë„ˆë¦¬ ë“±)
            print("âš ï¸ ê²°ê³¼ í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. Raw Dataë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            actual_test_results = []

        for result in actual_test_results:
            # resultëŠ” ì´ì œ 'TestResult' ê°ì²´ì…ë‹ˆë‹¤.
            
            # ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ”ì§€ ë°©ì–´ì  ì½”ë”©
            if not result.metrics_data:
                continue

            # ìš°ë¦¬ëŠ” metricì„ í•˜ë‚˜ë§Œ ë„£ì—ˆìœ¼ë¯€ë¡œ 0ë²ˆì§¸ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            metric_data = result.metrics_data[0] 
            clause_text = result.input
            
            # ì ìˆ˜ ê³„ì‚° (User Logic ìœ ì§€)
            # scoreëŠ” ë³´í†µ 0.0~1.0 ì‚¬ì´ë¡œ ë‚˜ì˜¤ë¯€ë¡œ 10ë°° í•´ì¤ë‹ˆë‹¤.
            risk_score = metric_data.score
            if risk_score <= 1.0:
                risk_score *= 10
            
            is_toxic = risk_score >= 4.0

            formatted_results.append({
                "clause": clause_text,
                "is_toxic": is_toxic,
                "risk_score": round(risk_score, 1),
                "reason": metric_data.reason,
                "context_used": original_map.get(clause_text, "")
            })

        return formatted_results

    def generate_easy_suggestion(self, detection_result):
        if not detection_result['is_toxic']:
            return "âœ… **ì•ˆì „í•œ ì¡°í•­ì…ë‹ˆë‹¤.**"

        prompt = f"""
        ë‹¹ì‹ ì€ ê·¼ë¡œì í¸ì¸ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë…ì†Œì¡°í•­ì„ ë¶„ì„í•˜ì„¸ìš”.
        
        [ì›ë¬¸]: {detection_result['clause']}
        [ì´ìœ ]: {detection_result['reason']}
        [ê·¼ê±°]: {detection_result['context_used']}

        ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì‘ì„±:
        1. **âš ï¸ ì‰¬ìš´ í•´ì„**: ê·¼ë¡œìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ 1~2ë¬¸ì¥ìœ¼ë¡œ 'ì™œ ìœ„í—˜í•œì§€' ì„¤ëª… ë° ìš”ì•½.
        2. **ğŸ’¡ ìˆ˜ì • ì œì•ˆ**: ë²•ì— ë§ëŠ” ê³µì •í•œ ì¡°í•­ ì˜ˆì‹œ.
        """
        response = self.llm_service.generate(prompt)
        return response.text if hasattr(response, 'text') else str(response)

# --- 3. í…ŒìŠ¤íŠ¸ ì½”ë“œ ---
if __name__ == "__main__":
    # os.environ["GEMINI_API_KEY"] = "YOUR_API_KEY"
    
    detector = ToxicClauseDetector()

    # í…ŒìŠ¤íŠ¸í•  ì¡°í•­ë“¤ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
    test_clauses = [
        "í‡´ì‚¬ ì‹œ í›„ì„ìë¥¼ êµ¬í•˜ì§€ ëª»í•˜ë©´ ì†í•´ë°°ìƒì„ ì²­êµ¬í•œë‹¤.", # ë…ì†Œì¡°í•­ (ë†’ì€ ì ìˆ˜ ì˜ˆìƒ)
        "ê·¼ë¡œì‹œê°„ì€ 09ì‹œë¶€í„° 18ì‹œê¹Œì§€ë¡œ í•œë‹¤.",              # ì •ìƒì¡°í•­ (ë‚®ì€ ì ìˆ˜ ì˜ˆìƒ)
        "ìˆ˜ìŠµê¸°ê°„ ì¤‘ì—ëŠ” ê¸‰ì—¬ì˜ 50%ë§Œ ì§€ê¸‰í•œë‹¤."               # ë…ì†Œì¡°í•­ (ìµœì €ì„ê¸ˆë²• ìœ„ë°˜ ê°€ëŠ¥ì„±)
    ]

    # ë³‘ë ¬ ì‹¤í–‰ (í•¨ìˆ˜ëª… detect ìœ ì§€)
    results = detector.detect(test_clauses, max_concurrent=3)

    print("\n" + "="*50)
    for res in results:
        status = "ğŸš¨ìœ„í—˜" if res['is_toxic'] else "âœ…ì•ˆì „"
        print(f"[{status}] ì ìˆ˜: {res['risk_score']} | ì¡°í•­: {res['clause'][:30]}...")
        print(f"   ì´ìœ : {res['reason']}")
        print("-" * 50)